{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f870fe15",
   "metadata": {},
   "source": [
    "# Vorbereitung Google Dev Board\n",
    "\n",
    "Lesen und folgen Sie Kapitel 3-4 des folgenden Links:\n",
    "\n",
    "https://coral.ai/docs/dev-board/get-started/#install-mdt\n",
    "\n",
    "Wenn Sie sich mit ihrem Board verbunden haben, können Sie mit dem alias \"coral\"\n",
    "die vordefinierte Pythonumgebung aktivieren.\n",
    "Mit:\n",
    "\n",
    "    jupyter-lab --ip=xxx.xxx.xxx.xxx\n",
    "    \n",
    "Können Sie jupyter-lab starten. Die Ip-adresse bekommen Sie mittels \"ip a\" heraus.\n",
    "Nun sollten Sie einen Link in der Kommandozeile finden, den Sie auf ihrem lokalen Rechner\n",
    "in den Browser eingeben können. \n",
    "\n",
        "### Achtung\n",
    "\n",
    "Das Google Dev Board ist an einen ssh key gebunden. Hieraus resultiert, dass sich nur ein Rechner per ssh verbinden kann.\n",
    "Bitte löschen Sie ihren Key nach der Benutzung des Boards mit dem Befehl  \n",
    "    \n",
    "    rm ~/.ssh/authorized_keys\n",
    "    \n",
    "oder installieren einen cronjob in dem Sie \n",
    "\n",
    "    (crontab -l; echo \"*/15 * * * * rm /home/mendel/.ssh/authorized_keys\")|awk '!x[$0]++'|crontab -\n",
    "    \n",
    "in die Kommandozeile des Google Dev Boards eingeben\n",
    "\n",
    "# Quantisierung\n",
    "\n",
    "Bevor wir ein Netzwerk auf dem Dev-Board ausführen können, müssen wir Besonderheiten beachten. \n",
    "Die Edge-TPU kann nur quantisierte Netzwerke ausfuehren. \n",
    "Wenn Sie ein NN auf ihrem lokalen Rechner trainieren, sind dessen Gewichte typischerweise im float32 format.\n",
    "Das Dev-Board verlangt Gewichte im uint8 format. Hier muss also eine Quantisierung von 32 bit auf 8 bit stattfinden. \n",
    "Sie Muessen ihr Netzwerk also quantisieren. \n",
    "Hierbei gibt es zwei Optionen:\n",
    "\n",
    "1. *Quantization-aware training*:  Trainingsschichten werden mittels ”fake” quantization nodes auf die 8-bit Gewichte der späteren Quantisierung vorbereitet.\n",
    "Daraus resultiert im allgemeinen eine höhere Genauigkeit im Vergleich zum\n",
    "post-training.\n",
    "\n",
    "2. *Full integer post-training quantization*:  Jedes beliebige NN kann nach dem\n",
    "training quantisiert werden. Es wird allerdings eine \"helperfunktion\" benötig,\n",
    "die der Quantisierungsroutine einen Beispielinput übergibt.\n",
    "\n",
    "Wir verwenden hier Variante 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Models!\n",
    "\n",
    "Modelname = \"Cnn\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(Modelname)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0e9f0",
   "metadata": {},
   "source": [
    " \n",
    " Wir benötigen nur ein paar Beispieldaten für die Quantisierung\n",
    " Hierbei wird der Prozess des Vorhersagens ein paar mal durchgespielt\n",
    " und die Gewichte demensprechend beschnitten\n",
    " Am Ende erhalten wir ein Model, das Gewichte im uint8 Format besitzt\n",
    " Vorteil hierbei ist, dass die Vorhersage schneller abgewickelt wird (Auf Kosten der Genauigkeit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantisierung\n",
    "\n",
    "modelname_quant = \"model_quant.tflite\"\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "X_train=x_train[0:100] / 255\n",
    "X_train=np.reshape(X_train, (X_train.shape[0],1,28,28,1))\n",
    "del x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def representative_data_gen_CNN():\n",
    "    for (label,img) in enumerate(X_train):     \n",
    "        yield [img.astype(np.float32)]\n",
    "\n",
    "\n",
    "# Kompiliertes und trainiertes Modell uebergeben\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "\n",
    "# Quantisierung aktivieren\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "# Hier den Generator einbinden\n",
    "converter.representative_dataset = representative_data_gen_CNN\n",
    "\n",
    "\n",
    "# Wirf Fehlermeldung, falls Operation nicht convertiert werden kann\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "\n",
    "# Quantisierung auf int8 setzen\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "\n",
    "# Input/Output auf uint8\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "\n",
    "# Konvertieren\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "# Speichern\n",
    "with open(modelname_quant, 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86186cd",
   "metadata": {},
   "source": [
    "## Edge-TPU-Compiler\n",
    "\n",
    "Für die Edge-TPU reicht dies allerdings noch nicht.\n",
    "\n",
    "Das Netzwerk muss erst noch durch den TPU-Compiler übersetzte werden.\n",
    "\n",
    "Leider gibt es den Compiler nur für Debian-basierte Betriebssysteme.\n",
    "\n",
    "Die Installation und Ausführung ist hier beschrieben:\n",
    "\n",
    "\n",
    "https://coral.ai/docs/edgetpu/compiler/#system-requirements\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db80799",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Alternative Dockerimage:\n",
    "\n",
    "\n",
    "Falls Sie kein Debian-basiertes System besitzen können Sie hierfür Docker verwenden.\n",
    "Installieren Sie Docker und führen Sie folgende Befehle in ihrem Notebook aus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe75d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inside_docker = \"\\\"cd data && edgetpu_compiler {modelname}\\\"\".format(modelname = modelname_quant)\n",
    "\n",
    "!docker run -v $(pwd):/data -it sichrist/edge_tpu_compiler bash -c {run_inside_docker}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e3f12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Das Modell kann innerhalb des Notebooks mit folgendem Befehl auf das Board gepusht werden:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mdt push {modelname_to_push} Pfad/zum/Verzeichnis/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5cd26",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    " Um Ihr Netzwerk auf dem Google Dev Board auszuführen, können Sie dieses \n",
    " [Notebook](https://github.com/oduerr/ki/blob/main/exercise_2/Mnist_Coral.ipynb) verwenden.\n",
    " \n",
    " Das Notebook finden Sie auch im Heimverzeichnis des Dev Boards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
